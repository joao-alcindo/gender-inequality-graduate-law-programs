{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaaab47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ce6b9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page URL: https://sucupira-legado.capes.gov.br/sucupira/public/consultas/coleta/envioColeta/dadosFotoEnvioColeta.xhtml\n",
      "Input file: ./data/test.csv\n",
      "Maximum retries: 5\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"general\": {\n",
    "        # True: Starts a new scraping process from the input spreadsheet.\n",
    "        # False: Attempts to reprocess failed items from the JSON file.\n",
    "        \"new_scraping\": True,\n",
    "    },\n",
    "    \"files\": {\n",
    "        # Path to the input spreadsheet (used if new_scraping is true).\n",
    "        #\"input_csv\": './data/ei_and_programs.csv',\n",
    "        \"input_csv\": './data/test.csv',\n",
    "\n",
    "        # Final file, with data enriched with descriptions.\n",
    "        \"final_enriched_json\": './data/sucupira_data_projects.json',\n",
    "\n",
    "        # File to save intermediate progress.\n",
    "        \"intermediate_json\": './dados_intermediate.json',\n",
    "    },\n",
    "    \"scrapers\": {\n",
    "        \"selenium\": {\n",
    "            # URL of the Sucupira platform's query page.\n",
    "            \"page_url\": \"https://sucupira-legado.capes.gov.br/sucupira/public/consultas/coleta/envioColeta/dadosFotoEnvioColeta.xhtml\",\n",
    "\n",
    "            # The value \"1000\" corresponds to \"All years\" in the page's form.\n",
    "            \"collection_calendar_value\": \"1000\",\n",
    "        },\n",
    "        \"beautifulsoup\": {\n",
    "            # Pause in seconds between each request to fetch the description.\n",
    "            # Helps to avoid overloading the website's server.\n",
    "            \"request_pause_seconds\": 1,\n",
    "        }\n",
    "    },\n",
    "    \"max_retries\": 5\n",
    "}\n",
    "\n",
    "# Example of how to access the values:\n",
    "print(f\"Page URL: {config['scrapers']['selenium']['page_url']}\")\n",
    "print(f\"Input file: {config['files']['input_csv']}\")\n",
    "print(f\"Maximum retries: {config['max_retries']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81441d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SucupiraScraper:\n",
    "    \"\"\"A web scraper for the Brazilian Sucupira academic platform.\n",
    "\n",
    "    This class automates the process of navigating the Sucupira platform,\n",
    "    searching for specific postgraduate programs, and extracting detailed\n",
    "    information, including program details and a full list of associated\n",
    "    research projects. It handles dynamic page elements, pagination, and\n",
    "    error logging.\n",
    "\n",
    "    Attributes:\n",
    "        config (dict): Configuration dictionary with URLs and other settings.\n",
    "        driver (webdriver): The Selenium Firefox WebDriver instance.\n",
    "        wait (WebDriverWait): Selenium wait object for handling dynamic elements.\n",
    "        collected_data (list): A list of dictionaries, where each dictionary\n",
    "            holds the scraped data for a successful program.\n",
    "        data_with_errors (list): A list of dictionaries for HEI/Program pairs\n",
    "            that failed during scraping.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initializes the SucupiraScraper instance.\n",
    "\n",
    "        Args:\n",
    "            config (dict): A configuration dictionary containing necessary\n",
    "                parameters like URLs and file paths.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        print(\"Initializing Selenium with Firefox...\")\n",
    "        \n",
    "        options = Options()\n",
    "        # Configurações para Firefox\n",
    "        options.add_argument(\"--width=1920\")\n",
    "        options.add_argument(\"--height=1080\")\n",
    "        # Descomente a linha abaixo se quiser executar em modo headless\n",
    "        # options.add_argument(\"--headless\")\n",
    "        \n",
    "        try:\n",
    "            # Usar webdriver-manager para gerenciar automaticamente o geckodriver\n",
    "            service = Service(GeckoDriverManager().install())\n",
    "            self.driver = webdriver.Firefox(service=service, options=options)\n",
    "            print(\"✅ Firefox WebDriver i\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro ao inicializar Firefox: {e}\")\n",
    "            print(\"Tentando usar geckodriver do sistema...\")\n",
    "            # Fallback: tentar usar geckodriver instalado via conda\n",
    "            self.driver = webdriver.Firefox(options=options)\n",
    "\n",
    "        self.wait = WebDriverWait(self.driver, 25)\n",
    "        self.collected_data = []\n",
    "        self.data_with_errors = []\n",
    "\n",
    "    def _select_hei_with_retry(self, hei_code, retries=2):\n",
    "        \"\"\"Attempts to select a Higher Education Institution (HEI) with retries.\n",
    "\n",
    "        This method handles the dynamic input field and selection list for the HEI\n",
    "        on the search form. It will try a specified number of times before failing.\n",
    "\n",
    "        Args:\n",
    "            hei_code (str): The code of the HEI to be selected.\n",
    "            retries (int): The number of attempts to make.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the HEI was selected successfully, False otherwise.\n",
    "        \"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                input_hei = self.wait.until(EC.element_to_be_clickable((By.ID, \"form:j_idt33:inst:input\")))\n",
    "                input_hei.clear()\n",
    "                input_hei.send_keys(hei_code)\n",
    "                \n",
    "                option_selector_xpath = f\"//select[@id='form:j_idt33:inst:listbox']/option[contains(text(), '{hei_code}')]\"\n",
    "                option = self.wait.until(EC.element_to_be_clickable((By.XPATH, option_selector_xpath)))\n",
    "                time.sleep(1.5)  # Wait for any potential JS event to settle\n",
    "                option.click()\n",
    "\n",
    "                print(\"-> Institution selected and VERIFIED successfully.\")\n",
    "                return True\n",
    "            \n",
    "            except TimeoutException:\n",
    "                print(f\"-> Attempt {attempt + 1}/{retries} failed to select the HEI. Retrying...\")\n",
    "                time.sleep(2)\n",
    "        return False\n",
    "\n",
    "    def _extract_header_info(self, label_text):\n",
    "        \"\"\"Extracts text from a div that follows a div containing a specific label.\n",
    "\n",
    "        A helper utility to scrape data from the header section of the program page,\n",
    "        which follows a 'label: value' pattern in the HTML structure.\n",
    "\n",
    "        Args:\n",
    "            label_text (str): The text of the <label> tag to find.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted text content, or \"Not found\" if the element\n",
    "                 could not be located.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            xpath = f\"//label[text()='{label_text}']/parent::div/following-sibling::div\"\n",
    "            return self.wait.until(EC.visibility_of_element_located((By.XPATH, xpath))).text.strip()\n",
    "        except TimeoutException:\n",
    "            return \"Not found\"\n",
    "\n",
    "    def _process_item(self, hei_code, program_code):\n",
    "        \"\"\"Processes a single HEI/Program pair to scrape its data.\n",
    "\n",
    "        This is the core scraping logic for one item. It navigates to the page,\n",
    "        fills out the form, extracts header information, and then scrapes all\n",
    "        associated research projects, handling pagination within the projects table.\n",
    "        Successful results are appended to `self.collected_data`, and failures\n",
    "        are logged in `self.data_with_errors`.\n",
    "\n",
    "        Args:\n",
    "            hei_code (str): The code for the Higher Education Institution.\n",
    "            program_code (str): The code for the postgraduate program.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.driver.get(self.config['scrapers']['selenium']['page_url'])\n",
    "            try:\n",
    "                cookie_button = WebDriverWait(self.driver, 5).until(EC.element_to_be_clickable((By.XPATH, \"//button[text()='ACEITO']\")))\n",
    "                cookie_button.click()\n",
    "            except TimeoutException:\n",
    "                pass  # Cookie button not found, proceed\n",
    "\n",
    "            calendar_value = self.config['scrapers']['selenium']['collection_calendar_value']\n",
    "            Select(self.wait.until(EC.element_to_be_clickable((By.ID, \"form:j_idt33:calendarioid\")))).select_by_value(calendar_value)\n",
    "\n",
    "            if not self._select_hei_with_retry(hei_code):\n",
    "                raise TimeoutException(f\"Could not select the HEI '{hei_code}' after multiple attempts.\")\n",
    "\n",
    "            program_selector_name = \"form:j_idt33:j_idt406\"\n",
    "            self.wait.until(lambda d: len(Select(d.find_element(By.NAME, program_selector_name)).options) > 1)\n",
    "            select_program = Select(self.driver.find_element(By.NAME, program_selector_name))\n",
    "            \n",
    "            value_to_select = next((opt.get_attribute(\"value\") for opt in select_program.options if program_code in opt.text), None)\n",
    "\n",
    "            if value_to_select:\n",
    "                select_program.select_by_value(value_to_select)\n",
    "            else:\n",
    "                raise NoSuchElementException(f\"Program '{program_code}' not found for HEI '{hei_code}'.\")\n",
    "            \n",
    "            self.wait.until(EC.element_to_be_clickable((By.ID, \"form:consultar\"))).click()\n",
    "            \n",
    "\n",
    "\n",
    "            # Click the \"Programa\" details button to ensure its content is loaded\n",
    "            program_details_button = self.wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'button[href=\"#collapsePrograma\"]')))\n",
    "            self.driver.execute_script(\"arguments[0].scrollIntoView(true); arguments[0].click();\", program_details_button)\n",
    "            time.sleep(1) # Small wait for the content to expand\n",
    "\n",
    "            program_info = {\n",
    "                'SEARCHED_HEI_CODE': hei_code,\n",
    "                'SEARCHED_PROGRAM_CODE': program_code,\n",
    "                'Institution': self._extract_header_info(\"Instituição de Ensino:\"),\n",
    "                'Program': self._extract_header_info(\"Programa:\"),\n",
    "                'Coordinator': self._extract_header_info(\"Coordenador(a):\"),\n",
    "                'Status': self._extract_header_info(\"Situação:\"),\n",
    "                'English_Name': self._extract_header_info(\"Nome em Inglês:\"),\n",
    "                'Basic_Area': self._extract_header_info(\"Área Básica:\"),\n",
    "                'Evaluation_Area': self._extract_header_info(\"Área de Avaliação:\"),\n",
    "                'Academic_Term': self._extract_header_info(\"Regime Letivo:\"),\n",
    "                'Modality': self._extract_header_info(\"Modalidade:\")\n",
    "            }\n",
    "\n",
    "            # Extract City and State from the \"Instituições de Ensino\" table\n",
    "            try:\n",
    "                # This XPath finds the table under the correct H1 and gets the 3rd and 4th columns of the first data row\n",
    "                # sigla em ingles\n",
    "                hei_abbreviation_xpath = \"//h1[contains(text(), 'Instituições de Ensino')]/following-sibling::div//table/tbody/tr/td[2]\"\n",
    "                city_xpath = \"//h1[contains(text(), 'Instituições de Ensino')]/following-sibling::div//table/tbody/tr/td[3]\"\n",
    "                state_xpath = \"//h1[contains(text(), 'Instituições de Ensino')]/following-sibling::div//table/tbody/tr/td[4]\"\n",
    "                \n",
    "                program_info['HEI_Abbreviation'] = self.wait.until(EC.visibility_of_element_located((By.XPATH, hei_abbreviation_xpath))).text.strip()\n",
    "                program_info['City'] = self.wait.until(EC.visibility_of_element_located((By.XPATH, city_xpath))).text.strip()\n",
    "                program_info['State_UF'] = self.wait.until(EC.visibility_of_element_located((By.XPATH, state_xpath))).text.strip()\n",
    "            except TimeoutException:\n",
    "                program_info['HEI_Abbreviation'] = \"Not found\"\n",
    "                program_info['City'] = \"Not found\"\n",
    "                program_info['State_UF'] = \"Not found\"\n",
    "\n",
    "\n",
    "            \n",
    "            projects_button = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'button[href=\"#collapseProjetos\"]')))\n",
    "            self.driver.execute_script(\"arguments[0].scrollIntoView(true); arguments[0].click();\", projects_button)\n",
    "\n",
    "            all_research_project_rows = []\n",
    "\n",
    "            def extract_data_from_current_page():\n",
    "                \"\"\"Helper function to extract project data from the currently visible page.\"\"\"\n",
    "                table_rows = self.wait.until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, \"#collapseProjetos table.table-bordered tbody tr\")))\n",
    "                for row in table_rows:\n",
    "                    try:\n",
    "                        columns = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                        if len(columns) < 6: continue\n",
    "                        \n",
    "                        details_link = columns[5].find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                        row_data = {\n",
    "                            'Project_Name': columns[0].text.strip(),\n",
    "                            'Research_Line': columns[1].text.strip(),\n",
    "                            'Concentration_Area': columns[2].text.strip(),\n",
    "                            'Project_Nature': columns[3].text.strip(),\n",
    "                            'Project_Status': columns[4].text.strip(),\n",
    "                            'Details_Link': details_link\n",
    "                        }\n",
    "                        all_research_project_rows.append(row_data)\n",
    "                    except (NoSuchElementException, IndexError) as e:\n",
    "                        print(f\"  - Warning: error processing a table row: {e}\")\n",
    "\n",
    "            # Pagination Logic\n",
    "            try:\n",
    "                pagination_selector_css = \"select[id$=':cmbPagina']\"\n",
    "                pagination_select_element = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, pagination_selector_css)))\n",
    "                pagination_select = Select(pagination_select_element)\n",
    "                \n",
    "                num_pages = len(pagination_select.options)\n",
    "                print(f\"-> Found {num_pages} pages of projects.\")\n",
    "\n",
    "                for i in range(num_pages):\n",
    "                    print(f\"  - Processing page {i + 1}/{num_pages}...\")\n",
    "                    \n",
    "                    if i > 0:\n",
    "                        old_first_row = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#collapseProjetos table.table-bordered tbody tr\")))\n",
    "                        pagination_select_element = self.driver.find_element(By.CSS_SELECTOR, pagination_selector_css)\n",
    "                        pagination_select = Select(pagination_select_element)\n",
    "                        pagination_select.select_by_index(i)\n",
    "                        self.wait.until(EC.staleness_of(old_first_row))\n",
    "\n",
    "                    extract_data_from_current_page()\n",
    "\n",
    "            except TimeoutException:\n",
    "                print(\"-> Only one page of projects found. Extracting data...\")\n",
    "                extract_data_from_current_page()\n",
    "\n",
    "            program_info['Projects'] = all_research_project_rows\n",
    "            self.collected_data.append(program_info)\n",
    "            print(f\"-> SUCCESS: Extracted {len(all_research_project_rows)} research projects from all pages.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"ERROR: {str(e).splitlines()[0]}\"\n",
    "            print(f\"-> {error_msg}\")\n",
    "            self.data_with_errors.append({\n",
    "                'CD_INSTITUICAO_ENSINO': hei_code,\n",
    "                'CD_PROGRAMA': program_code\n",
    "            })\n",
    "    def close_browser(self):\n",
    "        \"\"\"Closes the Selenium WebDriver instance.\n",
    "\n",
    "        This should be called at the end of the scraping process to free up\n",
    "        system resources.\n",
    "        \"\"\"\n",
    "        print(\"\\nClosing the browser...\")\n",
    "        self.driver.quit()\n",
    "\n",
    "    def run(self, uncollected_data=None):\n",
    "        \"\"\"Executes the main scraping loop.\n",
    "\n",
    "        This is the primary entry point for the scraper. It reads a list of\n",
    "        HEI/Program pairs from a CSV file (or a provided DataFrame) and iterates\n",
    "        through them, calling `_process_item` for each. It also handles\n",
    "        saving intermediate progress.\n",
    "\n",
    "        Args:\n",
    "            uncollected_data (pd.DataFrame, optional): A DataFrame containing\n",
    "                items that failed in a previous run. If provided, the scraper\n",
    "                will only process these items. Defaults to None, in which case\n",
    "                it reads from the initial input CSV file.\n",
    "\n",
    "        Returns:\n",
    "            tuple[list, list]: A tuple containing two lists:\n",
    "                - The first list contains all successfully collected data.\n",
    "                - The second list contains all items that resulted in an error.\n",
    "        \"\"\"\n",
    "        if uncollected_data is not None:\n",
    "            print(\"Restarting scraping for uncollected data...\")\n",
    "            search_df = uncollected_data\n",
    "        else:\n",
    "            search_df = pd.read_csv(self.config['files']['input_csv'], encoding='utf-8')\n",
    "\n",
    "        total = len(search_df)\n",
    "        print(f\"Starting the search for {total} unique HEI/Program pairs...\")\n",
    "\n",
    "        for index, row in search_df.iterrows():\n",
    "            hei_code = str(row['CD_INSTITUICAO_ENSINO'])\n",
    "            program_code = str(row['CD_PROGRAMA'])\n",
    "            print(f\"\\n({index + 1}/{total}) Processing HEI: {hei_code} | Program: {program_code}\")\n",
    "            self._process_item(hei_code, program_code)\n",
    "        \n",
    "            if (index + 1) % 10 == 0:\n",
    "                print(f\"\\nProgress: {index + 1}/{total} items processed.\")\n",
    "                print(f\"Collected so far: {len(self.collected_data)} items successfully.\")\n",
    "                print(f\"Items with errors so far: {len(self.data_with_errors)}\\n\")\n",
    "                with open(self.config['files']['intermediate_json'], 'w', encoding='utf-8') as f:\n",
    "                    json.dump(self.collected_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"\\nScraping finished. {len(self.collected_data)} items collected successfully.\")\n",
    "                                     \n",
    "        self.close_browser()\n",
    "        return self.collected_data, self.data_with_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4657186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the scraping process, including retries for failed items.\n",
    "    \"\"\"\n",
    "    # Maximum number of retry attempts for failed items.\n",
    "    MAX_RETRIES = config['max_retries']\n",
    "\n",
    "    print(\"--- STARTING STEP 1: SCRAPING RESEARCH PROJECTS (SELENIUM) ---\")\n",
    "\n",
    "    # Initial scraping run.\n",
    "    scraper = SucupiraScraper(config=config)\n",
    "    # The run method is called without arguments for the first run.\n",
    "    collected_data, failed_items = scraper.run()\n",
    "\n",
    "    # Loop to reprocess failed items from the initial run.\n",
    "    attempt_count = 0\n",
    "    # The loop continues as long as there are failed items and we haven't exceeded the retry limit.\n",
    "    while failed_items and attempt_count < MAX_RETRIES:\n",
    "        attempt_count += 1\n",
    "        print(f\"\\n--- RESTARTING SCRAPING (ATTEMPT {attempt_count}/{MAX_RETRIES}) FOR {len(failed_items)} FAILED ITEMS ---\")\n",
    "\n",
    "        time.sleep(5)  # Pause before retrying to avoid hammering the server.\n",
    "\n",
    "        # A new scraper instance is created for the retry attempt.\n",
    "        retry_scraper = SucupiraScraper(config=config)\n",
    "        retry_df = pd.DataFrame(failed_items)\n",
    "\n",
    "        # The run method is now called with the DataFrame of failed items.\n",
    "        newly_collected_data, failed_items = retry_scraper.run(uncollected_data=retry_df)\n",
    "\n",
    "        if newly_collected_data:\n",
    "            print(f\"Successfully collected {len(newly_collected_data)} new items.\")\n",
    "            # Add the newly collected data to the main list.\n",
    "            collected_data.extend(newly_collected_data)\n",
    "        else:\n",
    "            print(\"No new data was collected in this attempt.\")\n",
    "\n",
    "    # After all retries, check if there are still items that could not be processed.\n",
    "    if failed_items:\n",
    "        print(f\"After {MAX_RETRIES} attempts, {len(failed_items)} items still have errors and will be discarded.\")\n",
    "        # Optional: Save these persistent errors to a file for later analysis.\n",
    "        with open('persistent_errors_step1.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(failed_items, f, ensure_ascii=False, indent=4)\n",
    "            print(f\"Persistent errors saved to 'persistent_errors_step1.json'.\")\n",
    "\n",
    "    # Save all successfully collected data to a final JSON file.\n",
    "    print(f\"\\nSaving a total of {len(collected_data)} successfully collected items...\")\n",
    "    with open('intermediate_data.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(collected_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"\\nProcess finished. Final data saved to 'intermediate_data.json'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35ac40b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING STEP 1: SCRAPING RESEARCH PROJECTS (SELENIUM) ---\n",
      "Initializing Selenium with Firefox...\n",
      "✅ Firefox WebDriver i\n",
      "Starting the search for 11 unique HEI/Program pairs...\n",
      "\n",
      "(1/11) Processing HEI: 15004015 | Program: 15004015007P6\n",
      "-> ERROR: Message: The element with the reference 0084142c-5ca8-4c34-bd38-92fbd2ed6f95 is stale; either its node document is not the active document, or it is no longer connected to the DOM; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "\n",
      "(2/11) Processing HEI: 20001010 | Program: 20001010029P5\n",
      "-> ERROR: Message: The element with the reference 60bc3826-0be9-488e-9656-2759e37f3841 is stale; either its node document is not the active document, or it is no longer connected to the DOM; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "\n",
      "(3/11) Processing HEI: 25002015 | Program: 25002015006P5\n",
      "-> Institution selected and VERIFIED successfully.\n",
      "-> Found 1 pages of projects.\n",
      "  - Processing page 1/1...\n",
      "-> SUCCESS: Extracted 26 research projects from all pages.\n",
      "\n",
      "(4/11) Processing HEI: 27001016 | Program: 27001016035P4\n",
      "-> Institution selected and VERIFIED successfully.\n",
      "-> Found 1 pages of projects.\n",
      "  - Processing page 1/1...\n",
      "-> SUCCESS: Extracted 9 research projects from all pages.\n",
      "\n",
      "(5/11) Processing HEI: 28003012 | Program: 28003012007P6\n",
      "-> ERROR: Message: The element with the reference c7846921-b6c6-432d-9d8f-da0380fbeb47 is stale; either its node document is not the active document, or it is no longer connected to the DOM; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "\n",
      "(6/11) Processing HEI: 32001010 | Program: 32001010027P6\n",
      "-> ERROR: Message: The element with the reference 891bb2bb-29f2-4d22-9a8a-2e7e770e7dc0 is stale; either its node document is not the active document, or it is no longer connected to the DOM; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "\n",
      "(7/11) Processing HEI: 32053010 | Program: 32053010002P5\n",
      "-> Institution selected and VERIFIED successfully.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 13\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m scraper \u001b[38;5;241m=\u001b[39m SucupiraScraper(config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# The run method is called without arguments for the first run.\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m collected_data, failed_items \u001b[38;5;241m=\u001b[39m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Loop to reprocess failed items from the initial run.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m attempt_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[21], line 297\u001b[0m, in \u001b[0;36mSucupiraScraper.run\u001b[1;34m(self, uncollected_data)\u001b[0m\n\u001b[0;32m    295\u001b[0m program_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCD_PROGRAMA\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) Processing HEI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhei_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Program: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogram_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhei_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogram_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProgress: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m items processed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 158\u001b[0m, in \u001b[0;36mSucupiraScraper._process_item\u001b[1;34m(self, hei_code, program_code)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39melement_to_be_clickable((By\u001b[38;5;241m.\u001b[39mID, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mform:consultar\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# Click the \"Programa\" details button to ensure its content is loaded\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m program_details_button \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melement_to_be_clickable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCSS_SELECTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbutton[href=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m#collapsePrograma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments[0].scrollIntoView(true); arguments[0].click();\u001b[39m\u001b[38;5;124m\"\u001b[39m, program_details_button)\n\u001b[0;32m    160\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Small wait for the content to expand\u001b[39;00m\n",
      "File \u001b[1;32me:\\Miniconda3\\envs\\gender-inequality\\lib\\site-packages\\selenium\\webdriver\\support\\wait.py:145\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gender-inequality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
